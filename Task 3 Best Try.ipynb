{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize, StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21480, 5000)\n",
      "(17184, 5000)\n",
      "(4296, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_train=pd.read_csv('csv/train_tfidf_features.csv')#, skiprows=sorted(random.sample(range(17184), 17184-8000)));\n",
    "df_test=pd.read_csv('csv/test_tfidf_features.csv');\n",
    "\n",
    "df_train_dropped=df_train.drop(['id','label'], axis=1);\n",
    "df_y=df_train['label'];\n",
    "df_test_dropped = df_test.drop(['id'], axis=1);\n",
    "df_X=pd.concat([df_train_dropped,df_test_dropped]);\n",
    "print(df_X.shape)\n",
    "print(df_train_dropped.shape)\n",
    "print(df_test_dropped.shape)\n",
    "\n",
    "X=df_X.to_numpy()\n",
    "y=df_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = normalize(X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "bot = telebot.TeleBot(token=\"5436169433:AAG6B8ejgqrMy8klWdH5_o_5zFzgoTjK4N4\")\n",
    "chat_id = 1241494706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030649293959036\n",
      "0.7346523130637184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emeze\\AppData\\Local\\Temp\\ipykernel_30764\\1742293683.py:135: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  bot.send_message(chat_id, \"Training is done \" + (\"Fail\", \"Success\")[acc > 0.72])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<telebot.types.Message at 0x1d01194d240>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pca = PCA(n_components=2000)\n",
    "# X_embedded = pca.fit_transform(X)\n",
    "# X_embedded_train = X_embedded[:17184]\n",
    "# X_embedded_test = X_embedded[17184:]\n",
    "from cProfile import label\n",
    "from scipy import rand\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train_dropped.to_numpy(), y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "model = ExtraTreesClassifier(\n",
    "    n_estimators=500, \n",
    "    min_samples_split=10, \n",
    "    min_samples_leaf=1, \n",
    "    max_features='sqrt', \n",
    "    criterion='entropy', \n",
    "    bootstrap=False,\n",
    "    n_jobs=3,\n",
    "    random_state=0\n",
    ")\n",
    "clf = Pipeline(steps=[(\"scaler\", MinMaxScaler()), (\"model\", model)]).fit(X_train, y_train)\n",
    "\n",
    "# ExtraTrees\n",
    "# model = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "# params_grid = {\n",
    "#     'n_estimators':[100, 200, 300, 400, 500],\n",
    "#     'criterion': ['gini', 'log_loss', 'entropy'],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'max_features': ['log2', 'sqrt'],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "# }\n",
    "# clf = RandomizedSearchCV(model, params_grid, cv=3, n_iter=100, n_jobs=-1, scoring='f1_macro')\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# Best Estimator\n",
    "# {'n_estimators': 500,\n",
    "#  'min_samples_split': 10,\n",
    "#  'min_samples_leaf': 1,\n",
    "#  'max_features': 'sqrt',\n",
    "#  'criterion': 'entropy',\n",
    "#  'bootstrap': False}\n",
    "\n",
    "# # Tuned\n",
    "# f1M 0.7030649293959036\n",
    "# acc 0.7346523130637184\n",
    "\n",
    "# Plain\n",
    "# F1M 0.6997099322010205\n",
    "# ACC 0.7294151876636602\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "# clf = xgb.XGBClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# F1M 0.6634373048950617\n",
    "# ACC 0.7119581029967995\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_classifier.ipynb\n",
    "# select = SelectFromModel(LogisticRegression(class_weight='balanced', penalty='l2', C=0.01))\n",
    "# X_train = select.fit_transform(X_train, y_train)\n",
    "# X_test = select.transform(X_test)\n",
    "# clf = svm.SVC(class_weight='balanced', kernel='linear')\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# f1M 0.6939654193804721\n",
    "# ACC 0.7078847832411987\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Resampling\n",
    "# clf = svm.SVC(class_weight='balanced', kernel='linear')\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# X_train , y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# f1M 0.6832482825268418\n",
    "# ACC 0.6965376782077393\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "# clf = RandomForestClassifier(random_state=0)\n",
    "# params_grid = {\n",
    "#     'n_estimators':[100, 200, 300, 400],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "# }\n",
    "# clf = RandomizedSearchCV(model, params_grid, cv=3, n_iter=50, n_jobs=-1, scoring='f1_macro')\n",
    "# clf.fit(X_train, y_train)\n",
    "# Best params resulted\n",
    "# {'n_estimators': 300,\n",
    "#  'min_samples_split': 10,\n",
    "#  'min_samples_leaf': 2,\n",
    "#  'max_features': 'sqrt',\n",
    "#  'max_depth': None,\n",
    "#  'bootstrap': False}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# print(f1_score(y_train, y_preds))\n",
    "print(f1_score(y_test, y_preds, average='macro'))\n",
    "\n",
    "acc = accuracy_score(y_test, y_preds)\n",
    "print(acc)\n",
    "\n",
    "# output = pd.DataFrame(clf.predict(\n",
    "#     df_test_dropped.to_numpy())).set_index(df_test[\"id\"])\n",
    "\n",
    "# output\n",
    "# output.to_csv(\"outputPCASVM.csv\")\n",
    "bot.send_message(chat_id, \"Training is done \" + (\"Fail\", \"Success\")[acc > 0.72])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062176165803109"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(clf.predict(\n",
    "    df_test_dropped.to_numpy())).set_index(df_test[\"id\"])\n",
    "\n",
    "output\n",
    "output.to_csv(\"outputPCASVM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd04f83b62147083ad2379b53fd411988a943220ab0b1d1df224914b1237beb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
